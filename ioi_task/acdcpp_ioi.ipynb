{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../Automatic-Circuit-Discovery/')\n",
    "sys.path.append('..')\n",
    "import re\n",
    "\n",
    "import acdc\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Sentences from IOI vs ABC distribution                                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> IOI prompt                              </span>┃<span style=\"font-weight: bold\"> IOI subj </span>┃<span style=\"font-weight: bold\"> IOI indirect obj </span>┃<span style=\"font-weight: bold\"> ABC prompt                              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │ Jane     │ Victoria         │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │\n",
       "│ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │          │                  │ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │\n",
       "│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │          │                  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │ Sullivan │ Rose             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │\n",
       "│ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │          │                  │ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │\n",
       "│ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │          │                  │ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │ Alex     │ Alan             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │\n",
       "│ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │          │                  │ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │ Jessica  │ Crystal          │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │\n",
       "│ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │          │                  │ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │\n",
       "│ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │          │                  │ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │ Kevin    │ Jonathan         │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │\n",
       "│ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │          │                  │ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │\n",
       "│ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │          │                  │ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      Sentences from IOI vs ABC distribution                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIOI prompt                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI subj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI indirect obj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mABC prompt                             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │ Jane     │ Victoria         │ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │\n",
       "│ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │          │                  │ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │\n",
       "│ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │          │                  │ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │ Sullivan │ Rose             │ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │\n",
       "│ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │          │                  │ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │\n",
       "│ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │          │                  │ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │ Alex     │ Alan             │ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │\n",
       "│ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │          │                  │ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │ Jessica  │ Crystal          │ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │\n",
       "│ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │          │                  │ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │\n",
       "│ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │          │                  │ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │ Kevin    │ Jonathan         │ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │\n",
       "│ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │          │                  │ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │\n",
       "│ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │          │                  │ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ioi_dataset import IOIDataset, format_prompt, make_table\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')\n",
    "\n",
    "make_table(\n",
    "  colnames = [\"IOI prompt\", \"IOI subj\", \"IOI indirect obj\", \"ABC prompt\"],\n",
    "  cols = [\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "    model.to_string(clean_dataset.s_tokenIDs).split(),\n",
    "    model.to_string(clean_dataset.io_tokenIDs).split(),\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "  ],\n",
    "  title = \"Sentences from IOI vs ABC distribution\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6657f126",
   "metadata": {},
   "source": [
    "# Metric Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b9d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.8051605224609375, Corrupt direction: 1.747904658317566\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    io_logits = logits[range(logits.size(0)), ioi_dataset.word_idx['end'], ioi_dataset.io_tokenIDs]\n",
    "    s_logits = logits[range(logits.size(0)), ioi_dataset.word_idx['end'], ioi_dataset.s_tokenIDs]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(clean_dataset.toks)\n",
    "    corrupt_logits = model(corr_dataset.toks)\n",
    "    clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item()\n",
    "    corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item()\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -ioi_metric(logits)\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b08e9e-a140-4a97-a309-3210cc8f8ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_final.hook_normalized\n",
      "ln_final.hook_scale\n",
      "blocks.11.hook_resid_post\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.hook_mlp_in\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.attn.hook_result\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.hook_v_input\n",
      "blocks.11.hook_k_input\n",
      "blocks.11.hook_q_input\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.10.hook_resid_post\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.hook_mlp_in\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.attn.hook_result\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.hook_v_input\n",
      "blocks.10.hook_k_input\n",
      "blocks.10.hook_q_input\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.9.hook_resid_post\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.hook_mlp_in\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.attn.hook_result\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.hook_v_input\n",
      "blocks.9.hook_k_input\n",
      "blocks.9.hook_q_input\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.8.hook_resid_post\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.hook_mlp_in\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.attn.hook_result\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.hook_v_input\n",
      "blocks.8.hook_k_input\n",
      "blocks.8.hook_q_input\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.7.hook_resid_post\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.hook_mlp_in\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.attn.hook_result\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.hook_v_input\n",
      "blocks.7.hook_k_input\n",
      "blocks.7.hook_q_input\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.6.hook_resid_post\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.hook_mlp_in\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.attn.hook_result\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.hook_v_input\n",
      "blocks.6.hook_k_input\n",
      "blocks.6.hook_q_input\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.5.hook_resid_post\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.hook_mlp_in\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.attn.hook_result\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.hook_v_input\n",
      "blocks.5.hook_k_input\n",
      "blocks.5.hook_q_input\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.4.hook_resid_post\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.hook_mlp_in\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.attn.hook_result\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.hook_v_input\n",
      "blocks.4.hook_k_input\n",
      "blocks.4.hook_q_input\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.3.hook_resid_post\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.hook_mlp_in\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.attn.hook_result\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.hook_v_input\n",
      "blocks.3.hook_k_input\n",
      "blocks.3.hook_q_input\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.2.hook_resid_post\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.hook_mlp_in\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.attn.hook_result\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.hook_v_input\n",
      "blocks.2.hook_k_input\n",
      "blocks.2.hook_q_input\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.1.hook_resid_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.hook_mlp_in\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.attn.hook_result\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.hook_v_input\n",
      "blocks.1.hook_k_input\n",
      "blocks.1.hook_q_input\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.0.hook_resid_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.hook_mlp_in\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.attn.hook_result\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.hook_v_input\n",
      "blocks.0.hook_k_input\n",
      "blocks.0.hook_q_input\n",
      "blocks.0.hook_resid_pre\n",
      "hook_pos_embed\n",
      "hook_embed\n",
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69420/69420 [00:03<00:00, 18682.34it/s]\n",
      "  0%|          | 0/30 [08:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mreset_hooks()\n\u001b[1;32m      5\u001b[0m acdcpp_exp \u001b[39m=\u001b[39m ACDCPPExperiment(model,\n\u001b[1;32m      6\u001b[0m                               clean_dataset\u001b[39m.\u001b[39mtoks,\n\u001b[1;32m      7\u001b[0m                               corr_dataset\u001b[39m.\u001b[39mtoks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                               no_pruned_nodes_attr\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m                              )\n\u001b[0;32m---> 18\u001b[0m pruned_heads, num_passes, pruned_attrs \u001b[39m=\u001b[39m acdcpp_exp\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/acdcpp/ACDCPPExperiment.py:136\u001b[0m, in \u001b[0;36mACDCPPExperiment.run\u001b[0;34m(self, save_after_acdcpp, save_after_acdc)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving ACDC++ Graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    134\u001b[0m     show(exp\u001b[39m.\u001b[39mcorr, fname\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mims/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_name\u001b[39m}\u001b[39;00m\u001b[39m/thresh\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m_before_acdc.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m acdc_heads, passes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_acdc(exp)\n\u001b[1;32m    138\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving ACDC Graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m show(exp\u001b[39m.\u001b[39mcorr, fname\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mims/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_name\u001b[39m}\u001b[39;00m\u001b[39m/thresh\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m_after_acdc.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/acdcpp/ACDCPPExperiment.py:116\u001b[0m, in \u001b[0;36mACDCPPExperiment.run_acdc\u001b[0;34m(self, exp)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRunning ACDC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39mwhile\u001b[39;00m exp\u001b[39m.\u001b[39mcurrent_node:\n\u001b[0;32m--> 116\u001b[0m     exp\u001b[39m.\u001b[39;49mstep(testing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m (get_nodes(exp\u001b[39m.\u001b[39mcorr), exp\u001b[39m.\u001b[39mnum_passes)\n",
      "File \u001b[0;32m~/Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:597\u001b[0m, in \u001b[0;36mTLACDCExperiment.step\u001b[0;34m(self, early_stop, testing)\u001b[0m\n\u001b[1;32m    594\u001b[0m     old_second_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_second_metric\n\u001b[1;32m    596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_passes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 597\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_cur_metric(recalc_edges\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# warning: gives fast evaluation, though edge count is wrong\u001b[39;00m\n\u001b[1;32m    598\u001b[0m evaluated_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_metric \u001b[39m# self.metric(self.model(self.ds)) # OK, don't calculate second metric?\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m early_stop: \u001b[39m# for debugging the effects of one and only one forward pass WITH a corrupted edge\u001b[39;00m\n",
      "File \u001b[0;32m~/Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:206\u001b[0m, in \u001b[0;36mTLACDCExperiment.update_cur_metric\u001b[0;34m(self, recalc_metric, recalc_edges, initial)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_cur_metric\u001b[39m(\u001b[39mself\u001b[39m, recalc_metric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, recalc_edges\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, initial\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m recalc_metric:\n\u001b[0;32m--> 206\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds)\n\u001b[1;32m    207\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric(logits)\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecond_metric \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:405\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    402\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    403\u001b[0m         )\n\u001b[0;32m--> 405\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    406\u001b[0m         residual,\n\u001b[1;32m    407\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    408\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    410\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    411\u001b[0m         left_attention_mask\u001b[39m=\u001b[39;49mleft_attention_mask,\n\u001b[1;32m    412\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/transformer_lens/components.py:1066\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, left_attention_mask)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     normalized_resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(mlp_in)\n\u001b[1;32m   1063\u001b[0m     mlp_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_mlp_out(\n\u001b[1;32m   1064\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(normalized_resid_mid)\n\u001b[1;32m   1065\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     resid_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_post(\n\u001b[1;32m   1067\u001b[0m         resid_mid \u001b[39m+\u001b[39;49m mlp_out\n\u001b[1;32m   1068\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1070\u001b[0m     \u001b[39m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     \u001b[39m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     normalized_resid_pre_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(\n\u001b[1;32m   1073\u001b[0m         resid_pre\n\u001b[1;32m   1074\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39muse_hook_mlp_in\n\u001b[1;32m   1075\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_mlp_in(resid_pre\u001b[39m.\u001b[39mclone())\n\u001b[1;32m   1076\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/arthurenv/lib/python3.11/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:362\u001b[0m, in \u001b[0;36mTLACDCExperiment.receiver_hook\u001b[0;34m(self, hook_point_input, hook, verbose)\u001b[0m\n\u001b[1;32m    356\u001b[0m     hook_point_input[receiver_node_index\u001b[39m.\u001b[39mas_index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_cache\u001b[39m.\u001b[39monline_cache[\n\u001b[1;32m    357\u001b[0m         sender_node_name\n\u001b[1;32m    358\u001b[0m     ][sender_node_index\u001b[39m.\u001b[39mas_index]\u001b[39m.\u001b[39mto(hook_point_input\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    359\u001b[0m     \u001b[39m# Remove the effect of this head (from the corrupted data)\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     hook_point_input[receiver_node_index\u001b[39m.\u001b[39mas_index] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_cache\u001b[39m.\u001b[39;49mcorrupted_cache[\n\u001b[1;32m    361\u001b[0m         sender_node_name\n\u001b[0;32m--> 362\u001b[0m     ][sender_node_index\u001b[39m.\u001b[39;49mas_index]\u001b[39m.\u001b[39;49mto(hook_point_input\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    364\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown edge type \u001b[39m\u001b[39m{\u001b[39;00medge\u001b[39m.\u001b[39medge_type\u001b[39m}\u001b[39;00m\u001b[39m ... \u001b[39m\u001b[39m{\u001b[39;00medge\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "THRESHOLDS = np.arange(0.005, 0.155, 0.005)\n",
    "RUN_NAME = 'noabs_value'\n",
    "model.reset_hooks()\n",
    "acdcpp_exp = ACDCPPExperiment(model,\n",
    "                              clean_dataset.toks,\n",
    "                              corr_dataset.toks,\n",
    "                              ioi_metric,\n",
    "                              negative_ioi_metric,\n",
    "                              THRESHOLDS,\n",
    "                              run_name=RUN_NAME,\n",
    "                              verbose=False,\n",
    "                              attr_absolute_val=True,\n",
    "                              save_graphs_after=0.07,\n",
    "                              pruning_mode = \"edge\",\n",
    "                              no_pruned_nodes_attr=1,\n",
    "                              verbose=True,\n",
    "                             )\n",
    "pruned_heads, num_passes, pruned_attrs = acdcpp_exp.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3c0b5e5-7732-42da-b92e-687536aca96c",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdca38-9c1a-45ee-8625-93c06b569533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for thresh in pruned_heads.keys():\n",
    "   pruned_heads[thresh][0] = list(pruned_heads[thresh][0])\n",
    "   pruned_heads[thresh][1] = list(pruned_heads[thresh][1])\n",
    "\n",
    "cleaned_attrs = {}\n",
    "for thresh in pruned_attrs.keys():\n",
    "    cleaned_attrs[thresh] = {}\n",
    "    for (layer, head), attr in pruned_attrs[thresh].items():\n",
    "        cleaned_attrs[thresh][f'L{layer}H{head}'] = attr\n",
    "        \n",
    "with open(f'{RUN_NAME}_pruned_heads.json', 'w') as f:\n",
    "    json.dump(pruned_heads, f)\n",
    "with open(f'{RUN_NAME}_num_passes.json', 'w') as f:\n",
    "    json.dump(num_passes, f)\n",
    "with open(f'{RUN_NAME}_pruned_attrs.json', 'w') as f:\n",
    "    json.dump(cleaned_attrs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
