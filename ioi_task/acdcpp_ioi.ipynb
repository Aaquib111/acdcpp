{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../Automatic-Circuit-Discovery/')\n",
    "sys.path.append('..')\n",
    "import re\n",
    "\n",
    "import acdc\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Sentences from IOI vs ABC distribution                                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> IOI prompt                              </span>┃<span style=\"font-weight: bold\"> IOI subj </span>┃<span style=\"font-weight: bold\"> IOI indirect obj </span>┃<span style=\"font-weight: bold\"> ABC prompt                              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │ Jane     │ Victoria         │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │\n",
       "│ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │          │                  │ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │\n",
       "│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │          │                  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │ Sullivan │ Rose             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │\n",
       "│ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │          │                  │ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │\n",
       "│ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │          │                  │ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │ Alex     │ Alan             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │\n",
       "│ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │          │                  │ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │ Jessica  │ Crystal          │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │\n",
       "│ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │          │                  │ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │\n",
       "│ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │          │                  │ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │ Kevin    │ Jonathan         │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │\n",
       "│ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │          │                  │ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │\n",
       "│ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │          │                  │ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      Sentences from IOI vs ABC distribution                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIOI prompt                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI subj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI indirect obj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mABC prompt                             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │ Jane     │ Victoria         │ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │\n",
       "│ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │          │                  │ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │\n",
       "│ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │          │                  │ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │ Sullivan │ Rose             │ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │\n",
       "│ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │          │                  │ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │\n",
       "│ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │          │                  │ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │ Alex     │ Alan             │ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │\n",
       "│ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │          │                  │ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │ Jessica  │ Crystal          │ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │\n",
       "│ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │          │                  │ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │\n",
       "│ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │          │                  │ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │ Kevin    │ Jonathan         │ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │\n",
       "│ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │          │                  │ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │\n",
       "│ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │          │                  │ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ioi_dataset import IOIDataset, format_prompt, make_table\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')\n",
    "\n",
    "make_table(\n",
    "  colnames = [\"IOI prompt\", \"IOI subj\", \"IOI indirect obj\", \"ABC prompt\"],\n",
    "  cols = [\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "    model.to_string(clean_dataset.s_tokenIDs).split(),\n",
    "    model.to_string(clean_dataset.io_tokenIDs).split(),\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "  ],\n",
    "  title = \"Sentences from IOI vs ABC distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657f126",
   "metadata": {},
   "source": [
    "# Metric Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b9d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.8051648139953613, Corrupt direction: 1.4411938190460205\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    io_logits = logits[range(logits.size(0)), ioi_dataset.word_idx['end'], ioi_dataset.io_tokenIDs]\n",
    "    s_logits = logits[range(logits.size(0)), ioi_dataset.word_idx['end'], ioi_dataset.s_tokenIDs]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(clean_dataset.toks)\n",
    "    corrupt_logits = model(corr_dataset.toks)\n",
    "    clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item()\n",
    "    corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item()\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_abs_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -abs(ioi_metric(logits))\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b08e9e-a140-4a97-a309-3210cc8f8ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up model hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_final.hook_normalized\n",
      "ln_final.hook_scale\n",
      "blocks.11.hook_resid_post\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.hook_mlp_in\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.attn.hook_result\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.hook_v_input\n",
      "blocks.11.hook_k_input\n",
      "blocks.11.hook_q_input\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.10.hook_resid_post\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.hook_mlp_in\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.attn.hook_result\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.hook_v_input\n",
      "blocks.10.hook_k_input\n",
      "blocks.10.hook_q_input\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.9.hook_resid_post\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.hook_mlp_in\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.attn.hook_result\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.hook_v_input\n",
      "blocks.9.hook_k_input\n",
      "blocks.9.hook_q_input\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.8.hook_resid_post\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.hook_mlp_in\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.attn.hook_result\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.hook_v_input\n",
      "blocks.8.hook_k_input\n",
      "blocks.8.hook_q_input\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.7.hook_resid_post\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.hook_mlp_in\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.attn.hook_result\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.hook_v_input\n",
      "blocks.7.hook_k_input\n",
      "blocks.7.hook_q_input\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.6.hook_resid_post\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.hook_mlp_in\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.attn.hook_result\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.hook_v_input\n",
      "blocks.6.hook_k_input\n",
      "blocks.6.hook_q_input\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.5.hook_resid_post\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.hook_mlp_in\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.attn.hook_result\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.hook_v_input\n",
      "blocks.5.hook_k_input\n",
      "blocks.5.hook_q_input\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.4.hook_resid_post\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.hook_mlp_in\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.attn.hook_result\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.hook_v_input\n",
      "blocks.4.hook_k_input\n",
      "blocks.4.hook_q_input\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.3.hook_resid_post\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.hook_mlp_in\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.attn.hook_result\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.hook_v_input\n",
      "blocks.3.hook_k_input\n",
      "blocks.3.hook_q_input\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.2.hook_resid_post\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.hook_mlp_in\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.attn.hook_result\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.hook_v_input\n",
      "blocks.2.hook_k_input\n",
      "blocks.2.hook_q_input\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.1.hook_resid_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.hook_mlp_in\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.attn.hook_result\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.hook_v_input\n",
      "blocks.1.hook_k_input\n",
      "blocks.1.hook_q_input\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.0.hook_resid_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.hook_mlp_in\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.attn.hook_result\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.hook_v_input\n",
      "blocks.0.hook_k_input\n",
      "blocks.0.hook_q_input\n",
      "blocks.0.hook_resid_pre\n",
      "hook_pos_embed\n",
      "hook_embed\n",
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n",
      "Running ACDC++\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "PRUNING L0H2 with attribution 0.0015191561542451382\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H5 with attribution 0.0005826260894536972\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L0H7 with attribution 0.0016460426850244403\n",
      "\tFound node blocks.0.attn.hook_result\n",
      "\tRemoved node blocks.0.attn.hook_result\n",
      "PRUNING L1H3 with attribution 0.001511591486632824\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H8 with attribution 0.0002639004960656166\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L1H9 with attribution 0.001205438980832696\n",
      "\tFound node blocks.1.attn.hook_result\n",
      "\tRemoved node blocks.1.attn.hook_result\n",
      "PRUNING L3H11 with attribution 0.0003330218605697155\n",
      "\tFound node blocks.3.attn.hook_result\n",
      "\tRemoved node blocks.3.attn.hook_result\n",
      "PRUNING L4H2 with attribution 0.0016875413712114096\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L4H5 with attribution 0.0005987638141959906\n",
      "\tFound node blocks.4.attn.hook_result\n",
      "\tRemoved node blocks.4.attn.hook_result\n",
      "PRUNING L6H10 with attribution 0.001786523498594761\n",
      "\tFound node blocks.6.attn.hook_result\n",
      "\tRemoved node blocks.6.attn.hook_result\n",
      "PRUNING L7H2 with attribution 0.00020346330711618066\n",
      "\tFound node blocks.7.attn.hook_result\n",
      "\tRemoved node blocks.7.attn.hook_result\n",
      "PRUNING L8H1 with attribution 0.0013623485574498773\n",
      "\tFound node blocks.8.attn.hook_result\n",
      "\tRemoved node blocks.8.attn.hook_result\n",
      "PRUNING L11H11 with attribution 0.0008881660178303719\n",
      "\tFound node blocks.11.attn.hook_result\n",
      "\tRemoved node blocks.11.attn.hook_result\n",
      "Running ACDC\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 14.42 GiB already allocated; 10.19 MiB free; 14.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m RUN_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs_value\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m acdcpp_exp \u001b[38;5;241m=\u001b[39m ACDCPPExperiment(model,\n\u001b[1;32m      5\u001b[0m                               clean_dataset\u001b[38;5;241m.\u001b[39mtoks,\n\u001b[1;32m      6\u001b[0m                               corr_dataset\u001b[38;5;241m.\u001b[39mtoks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                               save_graphs_after\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     14\u001b[0m                              )\n\u001b[0;32m---> 15\u001b[0m pruned_heads, num_passes, pruned_attrs \u001b[38;5;241m=\u001b[39m \u001b[43macdcpp_exp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acdcpp/ioi_task/../ACDCPPExperiment.py:123\u001b[0m, in \u001b[0;36mACDCPPExperiment.run\u001b[0;34m(self, save_after_acdcpp, save_after_acdc)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving ACDC++ Graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m     show(exp\u001b[38;5;241m.\u001b[39mcorr, fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mims/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/thresh\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_before_acdc.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m acdc_heads, passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_acdc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_graphs_after:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving ACDC Graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/acdcpp/ioi_task/../ACDCPPExperiment.py:104\u001b[0m, in \u001b[0;36mACDCPPExperiment.run_acdc\u001b[0;34m(self, exp)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning ACDC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m exp\u001b[38;5;241m.\u001b[39mcurrent_node:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (get_nodes(exp\u001b[38;5;241m.\u001b[39mcorr), exp\u001b[38;5;241m.\u001b[39mnum_passes)\n",
      "File \u001b[0;32m~/acdcpp/ioi_task/../Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:642\u001b[0m, in \u001b[0;36mTLACDCExperiment.step\u001b[0;34m(self, early_stop, testing)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_wandb:\n\u001b[1;32m    633\u001b[0m         log_metrics_to_wandb(\n\u001b[1;32m    634\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    635\u001b[0m             current_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m             times \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[1;32m    640\u001b[0m         )\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_cur_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m testing:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/acdcpp/ioi_task/../Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:198\u001b[0m, in \u001b[0;36mTLACDCExperiment.update_cur_metric\u001b[0;34m(self, recalc_metric, recalc_edges, initial)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_cur_metric\u001b[39m(\u001b[38;5;28mself\u001b[39m, recalc_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, recalc_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recalc_metric:\n\u001b[0;32m--> 198\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric(logits)\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:405\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    402\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    403\u001b[0m         )\n\u001b[0;32m--> 405\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:1046\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, left_attention_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m   1039\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m   1044\u001b[0m         query_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(query_input)\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[0;32m-> 1046\u001b[0m         key_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m   1048\u001b[0m         value_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m   1049\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m   1050\u001b[0m         left_attention_mask\u001b[38;5;241m=\u001b[39mleft_attention_mask,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1054\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1055\u001b[0m         resid_pre \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m   1056\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:324\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m scale: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale(\n\u001b[1;32m    321\u001b[0m     (x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    323\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m scale  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_normalized(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 14.42 GiB already allocated; 10.19 MiB free; 14.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "THRESHOLDS = np.arange(0.002, 0.142, 0.002)\n",
    "RUN_NAME = 'abs_value'\n",
    "acdcpp_exp = ACDCPPExperiment(model,\n",
    "                              clean_dataset.toks,\n",
    "                              corr_dataset.toks,\n",
    "                              ioi_metric,\n",
    "                              negative_abs_ioi_metric,\n",
    "                              THRESHOLDS,\n",
    "                              run_name=RUN_NAME,\n",
    "                              verbose=True,\n",
    "                              attr_absolute_val=True,\n",
    "                              save_graphs_after=0.1,\n",
    "                             )\n",
    "pruned_heads, num_passes, pruned_attrs = acdcpp_exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759581c4-fb46-4537-bc4b-e0ba01d88a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0b5e5-7732-42da-b92e-687536aca96c",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdca38-9c1a-45ee-8625-93c06b569533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{run_name}_pruned_heads', 'w') as f:\n",
    "    json.dump(pruned_heads, f)\n",
    "with open(f'{run_name}_num_passes', 'w') as f:\n",
    "    json.dump(num_passes, f)\n",
    "with open(f'{run_name}_pruned_attrs', 'w') as f:\n",
    "    json.dump(pruned_attrs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
