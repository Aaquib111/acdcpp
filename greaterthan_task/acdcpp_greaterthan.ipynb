{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/canrager/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../Automatic-Circuit-Discovery/')\n",
    "sys.path.append('..')\n",
    "import re\n",
    "\n",
    "import acdc\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "from acdc.greaterthan.utils import get_all_greaterthan_things\n",
    "\n",
    "from ACDCPPExperiment import ACDCPPExperiment\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "\n",
      "Clean dataset samples\n",
      "The demonstrations lasted from the year 1267 to 12\n",
      "The assaults lasted from the year 1644 to 16\n",
      "The affair lasted from the year 1268 to 12\n",
      "The stature lasted from the year 1653 to 16\n",
      "The effort lasted from the year 1318 to 13\n",
      "\n",
      "Reference dataset samples\n",
      "The demonstrations lasted from the year 1201 to 12\n",
      "The assaults lasted from the year 1601 to 16\n",
      "The affair lasted from the year 1201 to 12\n",
      "The stature lasted from the year 1601 to 16\n",
      "The effort lasted from the year 1301 to 13\n"
     ]
    }
   ],
   "source": [
    "# Make clean dataset and reference dataset\n",
    "N = 25\n",
    "\n",
    "things = get_all_greaterthan_things(\n",
    "    num_examples=N, metric_name=\"greaterthan\", device=device\n",
    ")\n",
    "greaterthan_metric = things.validation_metric\n",
    "toks_int_values = things.validation_data # clean data x_i\n",
    "toks_int_values_other = things.validation_patch_data # corrupted data x_i'\n",
    "\n",
    "print(\"\\nClean dataset samples\")\n",
    "for i in range(5):\n",
    "    print(model.tokenizer.decode(toks_int_values[i]))\n",
    "\n",
    "print(\"\\nReference dataset samples\")\n",
    "for i in range(5):\n",
    "    print(model.tokenizer.decode(toks_int_values_other[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0255fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n",
       "       0.009, 0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017,\n",
       "       0.018, 0.019, 0.02 , 0.021, 0.022, 0.023, 0.024, 0.025, 0.026,\n",
       "       0.027, 0.028, 0.029, 0.03 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 0.031, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b08e9e-a140-4a97-a309-3210cc8f8ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n",
      "WARNING:root:cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_final.hook_normalized\n",
      "ln_final.hook_scale\n",
      "blocks.11.hook_resid_post\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.hook_mlp_in\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.attn.hook_result\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.hook_v_input\n",
      "blocks.11.hook_k_input\n",
      "blocks.11.hook_q_input\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.10.hook_resid_post\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.hook_mlp_in\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.attn.hook_result\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.hook_v_input\n",
      "blocks.10.hook_k_input\n",
      "blocks.10.hook_q_input\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.9.hook_resid_post\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.hook_mlp_in\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.attn.hook_result\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.hook_v_input\n",
      "blocks.9.hook_k_input\n",
      "blocks.9.hook_q_input\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.8.hook_resid_post\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.hook_mlp_in\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.attn.hook_result\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.hook_v_input\n",
      "blocks.8.hook_k_input\n",
      "blocks.8.hook_q_input\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.7.hook_resid_post\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.hook_mlp_in\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.attn.hook_result\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.hook_v_input\n",
      "blocks.7.hook_k_input\n",
      "blocks.7.hook_q_input\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.6.hook_resid_post\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.hook_mlp_in\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.attn.hook_result\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.hook_v_input\n",
      "blocks.6.hook_k_input\n",
      "blocks.6.hook_q_input\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.5.hook_resid_post\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.hook_mlp_in\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.attn.hook_result\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.hook_v_input\n",
      "blocks.5.hook_k_input\n",
      "blocks.5.hook_q_input\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.4.hook_resid_post\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.hook_mlp_in\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.attn.hook_result\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.hook_v_input\n",
      "blocks.4.hook_k_input\n",
      "blocks.4.hook_q_input\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.3.hook_resid_post\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.hook_mlp_in\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.attn.hook_result\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.hook_v_input\n",
      "blocks.3.hook_k_input\n",
      "blocks.3.hook_q_input\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.2.hook_resid_post\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.hook_mlp_in\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.attn.hook_result\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.hook_v_input\n",
      "blocks.2.hook_k_input\n",
      "blocks.2.hook_q_input\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.1.hook_resid_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.hook_mlp_in\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.attn.hook_result\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.hook_v_input\n",
      "blocks.1.hook_k_input\n",
      "blocks.1.hook_q_input\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.0.hook_resid_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.hook_mlp_in\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.attn.hook_result\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.hook_v_input\n",
      "blocks.0.hook_k_input\n",
      "blocks.0.hook_q_input\n",
      "blocks.0.hook_resid_pre\n",
      "hook_pos_embed\n",
      "hook_embed\n",
      "self.current_node=TLACDCInterpNode(blocks.11.hook_resid_post, [:])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69420/69420 [00:06<00:00, 10510.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ACDC++ Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [09:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m RUN_NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgreaterthan_first_pass\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m acdcpp_exp \u001b[39m=\u001b[39m ACDCPPExperiment(model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                               toks_int_values,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                               toks_int_values_other,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                               no_pruned_nodes_attr\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                              )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/canrager/acdcpp/greaterthan_task/acdcpp_greaterthan.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pruned_heads, num_passes, pruned_attrs \u001b[39m=\u001b[39m acdcpp_exp\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/acdcpp/greaterthan_task/../ACDCPPExperiment.py:139\u001b[0m, in \u001b[0;36mACDCPPExperiment.run\u001b[0;34m(self, save_after_acdcpp, save_after_acdc)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving ACDC++ Graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m     show(exp\u001b[39m.\u001b[39mcorr, fname\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mims/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_name\u001b[39m}\u001b[39;00m\u001b[39m/thresh\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m_before_acdc.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m acdc_heads, passes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_acdc(exp)\n\u001b[1;32m    141\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving ACDC Graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    142\u001b[0m show(exp\u001b[39m.\u001b[39mcorr, fname\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mims/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_name\u001b[39m}\u001b[39;00m\u001b[39m/thresh\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m_after_acdc.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/acdcpp/greaterthan_task/../ACDCPPExperiment.py:119\u001b[0m, in \u001b[0;36mACDCPPExperiment.run_acdc\u001b[0;34m(self, exp)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRunning ACDC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[39mwhile\u001b[39;00m exp\u001b[39m.\u001b[39mcurrent_node:\n\u001b[0;32m--> 119\u001b[0m     exp\u001b[39m.\u001b[39;49mstep(testing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m (get_nodes(exp\u001b[39m.\u001b[39mcorr), exp\u001b[39m.\u001b[39mnum_passes)\n",
      "File \u001b[0;32m~/acdcpp/Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:642\u001b[0m, in \u001b[0;36mTLACDCExperiment.step\u001b[0;34m(self, early_stop, testing)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musing_wandb:\n\u001b[1;32m    633\u001b[0m         log_metrics_to_wandb(\n\u001b[1;32m    634\u001b[0m             \u001b[39mself\u001b[39m,\n\u001b[1;32m    635\u001b[0m             current_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m             times \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime(),\n\u001b[1;32m    640\u001b[0m         )\n\u001b[0;32m--> 642\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_cur_metric()\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m testing:\n\u001b[1;32m    644\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/acdcpp/Automatic-Circuit-Discovery/acdc/TLACDCExperiment.py:198\u001b[0m, in \u001b[0;36mTLACDCExperiment.update_cur_metric\u001b[0;34m(self, recalc_metric, recalc_edges, initial)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_cur_metric\u001b[39m(\u001b[39mself\u001b[39m, recalc_metric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, recalc_edges\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, initial\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m recalc_metric:\n\u001b[0;32m--> 198\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds)\n\u001b[1;32m    199\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric(logits)\n\u001b[1;32m    200\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecond_metric \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:423\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    425\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/transformer_lens/components.py:61\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 61\u001b[0m         einsum(\n\u001b[1;32m     62\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     63\u001b[0m             residual,\n\u001b[1;32m     64\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     66\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_U\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/.pyenv/versions/acdc_env/envs/acdcpp_env/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "THRESHOLDS = np.arange(0, 0.031, 0.001)\n",
    "RUN_NAME = 'greaterthan_edge_e-3'\n",
    "acdcpp_exp = ACDCPPExperiment(model,\n",
    "                              toks_int_values,\n",
    "                              toks_int_values_other,\n",
    "                              greaterthan_metric,\n",
    "                              greaterthan_metric,\n",
    "                              THRESHOLDS,\n",
    "                              run_name=RUN_NAME,\n",
    "                              verbose=False,\n",
    "                              attr_absolute_val=True,\n",
    "                              save_graphs_after=0,\n",
    "                              pruning_mode=\"edge\",\n",
    "                              no_pruned_nodes_attr=1\n",
    "                             )\n",
    "pruned_heads, num_passes, pruned_attrs = acdcpp_exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0b5e5-7732-42da-b92e-687536aca96c",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdca38-9c1a-45ee-8625-93c06b569533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for thresh in pruned_heads.keys():\n",
    "   pruned_heads[thresh][0] = list(pruned_heads[thresh][0])\n",
    "   pruned_heads[thresh][1] = list(pruned_heads[thresh][1])\n",
    "\n",
    "cleaned_attrs = {}\n",
    "for thresh in pruned_attrs.keys():\n",
    "    cleaned_attrs[thresh] = {}\n",
    "    for (layer, head), attr in pruned_attrs[thresh].items():\n",
    "        cleaned_attrs[thresh][f'L{layer}H{head}'] = attr\n",
    "        \n",
    "with open(f'{RUN_NAME}_pruned_heads.json', 'w') as f:\n",
    "    json.dump(pruned_heads, f)\n",
    "with open(f'{RUN_NAME}_num_passes.json', 'w') as f:\n",
    "    json.dump(num_passes, f)\n",
    "with open(f'{RUN_NAME}_pruned_attrs.json', 'w') as f:\n",
    "    json.dump(cleaned_attrs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
