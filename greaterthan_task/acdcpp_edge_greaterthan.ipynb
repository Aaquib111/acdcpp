{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/nyh0947d6sngjfvtp7hbb0jr0000gn/T/ipykernel_73878/3115277231.py:4: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%load_ext autoreload\")\n",
      "/var/folders/tn/nyh0947d6sngjfvtp7hbb0jr0000gn/T/ipykernel_73878/3115277231.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.magic(\"%load_ext autoreload\")\n",
    "    ipython.magic(\"%autoreload 2\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../Automatic-Circuit-Discovery/')\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import acdc\n",
    "from utils.prune_utils import get_3_caches, split_layers_and_heads\n",
    "from acdc.greaterthan.utils import get_all_greaterthan_things\n",
    "from acdc.TLACDCExperiment import TLACDCExperiment\n",
    "from acdc.acdc_utils import TorchIndex, EdgeType\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "\n",
    "from jaxtyping import Float, Bool\n",
    "from typing import Callable, Tuple, Union, Dict, Optional\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Metric & Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "\n",
      "Clean dataset samples\n",
      "The demonstrations lasted from the year 1267 to 12\n",
      "The assaults lasted from the year 1644 to 16\n",
      "The affair lasted from the year 1268 to 12\n",
      "The stature lasted from the year 1653 to 16\n",
      "The effort lasted from the year 1318 to 13\n",
      "\n",
      "Reference dataset samples\n",
      "The demonstrations lasted from the year 1201 to 12\n",
      "The assaults lasted from the year 1601 to 16\n",
      "The affair lasted from the year 1201 to 12\n",
      "The stature lasted from the year 1601 to 16\n",
      "The effort lasted from the year 1301 to 13\n"
     ]
    }
   ],
   "source": [
    "# Make clean dataset and reference dataset\n",
    "N = 25\n",
    "\n",
    "things = get_all_greaterthan_things(\n",
    "    num_examples=N, metric_name=\"greaterthan\", device=device\n",
    ")\n",
    "greaterthan_metric = things.validation_metric\n",
    "clean_ds = things.validation_data # clean data x_i\n",
    "corr_ds = things.validation_patch_data # corrupted data x_i'\n",
    "\n",
    "print(\"\\nClean dataset samples\")\n",
    "for i in range(5):\n",
    "    print(model.tokenizer.decode(clean_ds[i]))\n",
    "\n",
    "print(\"\\nReference dataset samples\")\n",
    "for i in range(5):\n",
    "    print(model.tokenizer.decode(corr_ds[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56b08e9e-a140-4a97-a309-3210cc8f8ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the 2 fwd and 1 bwd caches; cache \"normalized\" and \"result\" of attn layers\n",
    "clean_cache, corrupted_cache, clean_grad_cache = get_3_caches(\n",
    "    model, \n",
    "    clean_ds,\n",
    "    corr_ds,\n",
    "    metric=greaterthan_metric,\n",
    "    mode = \"edge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50407fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_head_act = split_layers_and_heads(clean_cache.stack_head_results(), model=model)\n",
    "corr_head_act = split_layers_and_heads(corrupted_cache.stack_head_results(), model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0112ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_grad_act = torch.zeros(\n",
    "    3, # QKV\n",
    "    model.cfg.n_layers,\n",
    "    model.cfg.n_heads,\n",
    "    clean_head_act.shape[-3], # Batch\n",
    "    clean_head_act.shape[-2], # Seq\n",
    "    clean_head_act.shape[-1], # D\n",
    ")\n",
    "\n",
    "for letter_idx, letter in enumerate(\"qkv\"):\n",
    "    for layer_idx in range(model.cfg.n_layers):\n",
    "        stacked_grad_act[letter_idx, layer_idx] = einops.rearrange(clean_grad_cache[f\"blocks.{layer_idx}.hook_{letter}_input\"], \"batch seq n_heads d -> n_heads batch seq d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d4f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for upstream_layer_idx in range(model.cfg.n_layers):\n",
    "    for upstream_head_idx in range(model.cfg.n_heads):\n",
    "        for downstream_letter_idx, downstream_letter in enumerate(\"qkv\"):\n",
    "            for downstream_layer_idx in range(upstream_layer_idx+1, model.cfg.n_layers):\n",
    "                for downstream_head_idx in range(model.cfg.n_heads):\n",
    "                    results[\n",
    "                        (\n",
    "                            upstream_layer_idx,\n",
    "                            upstream_head_idx,\n",
    "                            downstream_letter,\n",
    "                            downstream_layer_idx,\n",
    "                            downstream_head_idx,\n",
    "                        )\n",
    "                    ] = (stacked_grad_act[downstream_letter_idx, downstream_layer_idx, downstream_head_idx].cpu() * (clean_head_act[upstream_layer_idx, upstream_head_idx] - corr_head_act[upstream_layer_idx, upstream_head_idx]).cpu()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140a6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results.items(), key=lambda x: x[1].abs(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab2dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important edges:\n",
      "9:9 -> 11:10\n",
      "10:7 -> 11:10\n",
      "5:5 -> 8:6\n",
      "9:9 -> 10:7\n",
      "5:5 -> 6:9\n",
      "9:6 -> 11:10\n",
      "4:11 -> 6:9\n",
      "9:6 -> 10:7\n",
      "5:5 -> 7:9\n",
      "10:10 -> 11:10\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 most important edges:\")\n",
    "for i in range(10):\n",
    "    print(\n",
    "        f\"{sorted_results[i][0][0]}:{sorted_results[i][0][1]} -> {sorted_results[i][0][3]}:{sorted_results[i][0][4]}\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
